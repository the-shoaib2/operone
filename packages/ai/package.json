{
    "name": "@operone/ai",
    "version": "0.1.0",
    "description": "AI model orchestration and local inference for Operone",
    "main": "dist/index.js",
    "types": "dist/index.d.ts",
    "scripts": {
        "build": "tsc",
        "dev": "tsc --watch",
        "test": "vitest run",
        "lint": "eslint src/",
        "check-types": "tsc --noEmit"
    },
    "dependencies": {
        "@llama-node/llama-cpp": "^0.1.6",
        "@ai-sdk/provider": "^1.0.0",
        "@operone/core": "workspace:*",
        "@repo/types": "workspace:*",
        "onnxruntime-node": "^1.23.2"
    },
    "devDependencies": {
        "@repo/eslint-config": "workspace:*",
        "@types/node": "^20.19.25",
        "typescript": "5.9.3",
        "vitest": "^2.1.9"
    },
    "keywords": [
        "operone",
        "ai",
        "inference",
        "llama",
        "onnx"
    ],
    "author": "Operone Team",
    "license": "MIT"
}